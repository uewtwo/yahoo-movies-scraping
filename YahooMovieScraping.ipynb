{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "発火点は\n",
    "・allScrapings\n",
    "・reaggregateNotColsedByCsv\n",
    "\n",
    "allScrapings は実行するだけで Yahoo!映画から全件取得して csv に書き出す.\n",
    "\n",
    "reaggregate は published_state が上映終了ではない作品の再集計を行う\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_URL = \"https://movies.yahoo.co.jp/movie/\"\n",
    "PAST_QUERY = \"?&roadshow_flg=0&img_type=2&sort=year\"\n",
    "LATEST_QUERY = \"?&roadshow_flg=0&img_type=2&sort=-year\"\n",
    "# 上映ステータス\n",
    "CLOSED = 0\n",
    "ON_AIR = 1\n",
    "COMING_SOON = 5\n",
    "NOT_PRODUCT = 100\n",
    "\n",
    "# ディレイ\n",
    "REQUEST_DELAY = 1\n",
    "\n",
    "# デフォルトターゲットファイル\n",
    "TARGET_FILE = 'yahoo-movies-output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n残りタスク\\nページング\\n各評価点\\n評価数\\nキャスト\\nスケジュール（何をとるかは決めていない）\\n公開ステータス取得\\nまず、あらすじがあるかないか判断する\\nあらすじがなく、ラベルがない場合は製作だけ決まっているので更新対象\\nあらすじがあり、ラベルがない場合は上映が終わっているのでFIX\\nラベルがある場合は、ラベルによって更新するものを変更する\\npublished_state: [\\n    0,    #上映終了\\n    1,    #上映中\\n    5,    #上映予定\\n    100,  #未製作\\n]\\n余裕があったら必要項目の外出しとかしたい\\n'"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "published_state: [\n",
    "    0,    #上映終了\n",
    "    1,    #上映中\n",
    "    5,    #上映予定\n",
    "    100,  #未製作\n",
    "]\n",
    "余裕があったら必要項目の外出しとかしたい\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping 関数\n",
    "# トップレベルの情報を集めるやつ\n",
    "def scrapingTop(page, is_past=False):\n",
    "    # set\n",
    "    url = BASE_URL\n",
    "    if is_past:\n",
    "        url += PAST_QUERY\n",
    "    else:\n",
    "        url += LATEST_QUERY\n",
    "    \n",
    "    url +=  '&page=' + str(page)\n",
    "    soup = getSoupWithDelay(url)\n",
    "    return parseProductWithNest(soup) \n",
    "\n",
    "# トップページのサムネ群のパース\n",
    "# thumbnail--movies 以下が渡って来ないと多分困る\n",
    "def parseProductWithNest(soup):\n",
    "    thums = soup.find('ul', class_ = 'thumbnail--movies')\n",
    "    movies = thums.find_all('li', class_='col')\n",
    "    \n",
    "    # id , title, 制作年度, 制作国, ジャンルs, イメージワード, 監督, 原作 をひとまとまりにする.\n",
    "    products = {}\n",
    "    # 作品トップ\n",
    "    for movie in movies:\n",
    "        movie_id, product, product_soup = parseProductTopByThumbSoup(movie)\n",
    "        # 未製作以外は詳細情報を集める\n",
    "        # 上映中と上映終了は同じデータを集める\n",
    "        # 未上映であれば、情報を絞って集める\n",
    "        if product['published_state'] < NOT_PRODUCT:\n",
    "            # 製作が済んでいる\n",
    "            movie_details = getMovieDetailsOnProduct(product_soup, product['product_url'])\n",
    "            product.update(movie_details)\n",
    "            print('ONPURODUCT: ', product['title'])\n",
    "        else:\n",
    "            # 未製作映画\n",
    "            print('未製作映画', product['title'])\n",
    "        \n",
    "        _product = {}\n",
    "        for s, t in product.items():\n",
    "            _product.update({\n",
    "                s: convert2Json(t)\n",
    "            })\n",
    "        products[movie_id] = _product\n",
    "\n",
    "    # 次ページがあるか判定\n",
    "    existNextPage = True\n",
    "    next_page = soup.find('ul', class_='pagination').find_all('li')[-1]\n",
    "    try:\n",
    "        if next_page['class'][0] == 'is-disabled':\n",
    "            # 最後のページ\n",
    "            existNextPage = False\n",
    "        else:\n",
    "            existNextPage = True\n",
    "    except:\n",
    "        existNextPage = True\n",
    "    # movie 詳細　+ トップページの次のページがあるか\n",
    "    return products, existNextPage\n",
    "\n",
    "# 作品トップレベル解析\n",
    "# 個別の作品のデータ取得\n",
    "def parseProductTopByThumbSoup(movie):\n",
    "    # thumbnails 一覧ページでわかる情報\n",
    "    movie_info = {}\n",
    "    # ユニークID\n",
    "    movie_id = movie.get('data-cinema-id')\n",
    "    # タイトル\n",
    "    title = movie.h3.get('title')\n",
    "    # 制作年度\n",
    "    production_year = int(movie.small.text[1:5])\n",
    "    \n",
    "    # 作品トップへ\n",
    "    target_url = BASE_URL + title + '/' + movie_id + '/'\n",
    "    soup = getSoupWithDelay(target_url)\n",
    "    # 作品のステータス判定\n",
    "    published_state = getPublishedState(soup)\n",
    "    \n",
    "    # 一旦まとめて辞書更新\n",
    "    movie_info.update(\n",
    "        movie_id = int(movie_id),\n",
    "        title = title,\n",
    "        production_year = production_year,\n",
    "        published_state = published_state,\n",
    "        product_url = target_url\n",
    "    )\n",
    "    \n",
    "    # 作品トップの情報\n",
    "    product_info = getProductDetails(soup)\n",
    "    movie_info.update(product_info)\n",
    "    \n",
    "    return movie_id, movie_info, soup\n",
    "\n",
    "# 作品URLで解析する\n",
    "# 作品 dictionary を返す\n",
    "def parseProductWithNestByUrl(target_url):\n",
    "    soup = getSoupWithDelay(target_url)\n",
    "    movie_id, product = parseProductTopByTargetSoup(soup)\n",
    "    \n",
    "    if product['published_state'] < NOT_PRODUCT:\n",
    "        # 製作が済んでいる\n",
    "        movie_details = getMovieDetailsOnProduct(soup, product['product_url'])\n",
    "        product.update(movie_details)\n",
    "        print('ONPURODUCT: ', product['title'])\n",
    "    else:\n",
    "        # 未製作映画\n",
    "        print('未製作映画', product['title'])\n",
    "    \n",
    "    return product\n",
    "\n",
    "# 作品トップレベル解析\n",
    "# 作品トップの soup を受け取って解析する\n",
    "def parseProductTopByTargetSoup(soup):\n",
    "    movie_info = {}\n",
    "    '''\n",
    "    # movie_id\n",
    "    # title\n",
    "    # production_year\n",
    "    # の取得\n",
    "    '''\n",
    "    published_state = getPublishedState(soup)\n",
    "    movie_info.update(\n",
    "        movie_id = int(movie_id),\n",
    "        title = title,\n",
    "        production_year = production_year,\n",
    "        published_state = published_state,\n",
    "        product_url = target_url\n",
    "    )\n",
    "    \n",
    "    product_info = getProductDetails(soup)\n",
    "    movie_info.update(product_info)\n",
    "    \n",
    "    return movie_id, movie_info\n",
    "\n",
    "# 映画トップページ Soup を受けとって、公開ステータスを返す.\n",
    "def getPublishedState(soup):\n",
    "    #  movie description がない場合は未製作とみなす!! 決定!!\n",
    "    description = soup.find('section', class_='movie_description')\n",
    "    if description is None:\n",
    "        return NOT_PRODUCT\n",
    "    \n",
    "    # 製作はされているので、ラベルがあるか判定する\n",
    "    if soup.find('span', class_='label bgcolor-B') is not None:\n",
    "        # blue ラベルなので上映予定\n",
    "        return COMING_SOON\n",
    "    elif soup.find('p', class_='icon_showing label bgcolor-YR') is not None:\n",
    "        # 上映中\n",
    "        return ON_AIR\n",
    "    \n",
    "    # 規定ラベルがないので、上映終了と判断\n",
    "    return CLOSED\n",
    "        \n",
    "\n",
    "# 作品トップのプロダクト概要を集める\n",
    "def getProductDetails(soup):\n",
    "    # 作品情報解析\n",
    "    tbody = soup.find('tbody')\n",
    "    details = tbody.find_all('tr')\n",
    "\n",
    "    single_target = ['原題', '製作国', '上映時間']\n",
    "    multi_target = ['ジャンル', '原作', '脚本', '音楽']\n",
    "    product_info = {}\n",
    "    \n",
    "    for detail in details:\n",
    "        tar = detail.find('th').text\n",
    "\n",
    "        # 単独であろう項目群\n",
    "        if tar in single_target:\n",
    "            value = detail.find('td').text\n",
    "            product_info[jpn2Eng(tar)] = value\n",
    "        # 複数ありえる項目群\n",
    "        elif tar in multi_target:\n",
    "            value_list = detail.find_all('li')\n",
    "            values = list(map(lambda s: s.text, value_list))\n",
    "            product_info[jpn2Eng(tar)] = values\n",
    "    \n",
    "    return product_info\n",
    "\n",
    "\n",
    "# 製作されているっぽいやつ向けの情報収集機\n",
    "def getMovieDetailsOnProduct(soup, target_url):\n",
    "    _movie_details = _getMovieDetailsOnProduct(soup, target_url)\n",
    "    return _movie_details\n",
    "\n",
    "# まとめて操作するやつ\n",
    "def _getMovieDetailsOnProduct(soup, target_url):\n",
    "        _movie_details = {}\n",
    "        \n",
    "        # 詳細取得関数を呼んでマージしていく\n",
    "        # ページ遷移なし\n",
    "        _movie_details.update(getImageWords(soup))\n",
    "        _movie_details.update(getReviewPointAvg(soup))\n",
    "        _movie_details.update(getReviewPointDistribution(soup))\n",
    "        _movie_details.update(getReviewPointChart(soup))\n",
    "        \n",
    "        # ページ遷移あり\n",
    "        _movie_details.update(getRundown(soup, target_url))\n",
    "        _movie_details.update(getCredit(soup, target_url))\n",
    "        \n",
    "        return _movie_details\n",
    "\n",
    "'''\n",
    "ページ遷移なし\n",
    "'''\n",
    "# イメージワード\n",
    "# 作品トップを受け取る\n",
    "def getImageWords(soup):\n",
    "    image_word_list = soup.find('ul', class_='image_words list-inline text-xsmall').find_all('li')\n",
    "    image_words =  list(map(lambda s: s.text, image_word_list))\n",
    "    return {'image_words': image_words}\n",
    "# レビューの平均点\n",
    "# 作品トップを受け取る\n",
    "def getReviewPointAvg(soup):\n",
    "    try:\n",
    "        review_point_avg = float(soup.find('span', itemprop='ratingValue').text)\n",
    "    except:\n",
    "        # 未評価がある\n",
    "        review_point_avg = None\n",
    "    return {'review_point_avg': review_point_avg}\n",
    "# レビューの星得点割合\n",
    "# 作品トップを受け取る\n",
    "def getReviewPointDistribution(soup):\n",
    "    distribution_list = soup.find(\n",
    "        'ul', class_='rating-distribution text-xsmall').find_all('div', class_='rating-distribution__cell-point')\n",
    "    # 星に対応\n",
    "    rate_index = [5, 4, 3, 2, 1]\n",
    "    distributions = list(map(lambda s: convertToFloat(s.text[:-1]), distribution_list))\n",
    "    review_point_rate = dict(zip(rate_index, distributions))\n",
    "    return {'review_point_distribution': review_point_rate}\n",
    "# 得点分布\n",
    "# 作品トップを受け取る\n",
    "def getReviewPointChart(soup):\n",
    "    chart = soup.find('canvas', class_='rader-chart__figure')\n",
    "    label = chart['data-chart-label']\n",
    "    val = chart['data-chart-val-total']\n",
    "    review_point_distribution = dict(zip(label.split(','), list(map(float, val.split(',')))))\n",
    "    return {'review_point_chart': review_point_distribution}\n",
    "\n",
    "# 数字文字列を受けとって float にして返す\n",
    "# 空文字列だった場合は None にする\n",
    "def convertToFloat(num_string):\n",
    "    if num_string == '':\n",
    "        return None\n",
    "    else:\n",
    "        return float(num_string)\n",
    "\n",
    "'''\n",
    "ページ遷移あり\n",
    "''' \n",
    "# 解説, あらすじ\n",
    "# 作品トップを受け取る\n",
    "def getRundown(soup, target_url):\n",
    "    story_url = target_url + 'story'\n",
    "    soup = getSoupWithDelay(story_url)\n",
    "    sections = soup.find('div', id='story').find_all('section', class_='section')\n",
    "    \n",
    "    rundowns = {}\n",
    "    for section in sections:\n",
    "        category = \"\"\n",
    "        if section.find('h2', class_='text-middle').text == '解説':\n",
    "            category = jpn2Eng('解説')\n",
    "        elif section.find('h2', class_='text-middle').text == 'あらすじ':\n",
    "            category = jpn2Eng('あらすじ')\n",
    "        elif section.find('h2', class_='text-middle').text == '映画レポート':\n",
    "            category = jpn2Eng('映画レポート')\n",
    "        else:\n",
    "            raise ValueError(\"想定外のストーリー内セクション\")\n",
    "        \n",
    "        rundown = {\n",
    "            category: section.find('p', class_='text-readable').contents[0].strip()\n",
    "        }\n",
    "        rundowns.update(rundown)\n",
    "\n",
    "    return rundowns\n",
    "\n",
    "# クレジット\n",
    "# 作品トップを受け取る\n",
    "def getCredit(soup, target_url):\n",
    "    credit_url = target_url + 'credit'\n",
    "    soup = getSoupWithDelay(credit_url)\n",
    "    \n",
    "    # キャスト\n",
    "    casts = []\n",
    "    try:\n",
    "        cast_list = soup.find('div', id='cstl').find_all('h3')\n",
    "        for cast in cast_list:\n",
    "            casts.append(cast.text)\n",
    "    except AttributeError:\n",
    "        casts = None\n",
    "\n",
    "    # スタッフ\n",
    "    staffs = []\n",
    "    try:\n",
    "        staff_list = soup.find('div', id='stfl').find_all('div', class_='box__cell pl1em')\n",
    "        for staff in staff_list:\n",
    "            name = staff.find('h3', class_='text-middle text-break color-sub').text\n",
    "            try:\n",
    "                position = d.find('p', class_='text-xsmall no-space-bottom').text\n",
    "            except AttributeError:\n",
    "                position = None\n",
    "            _staff = {\n",
    "                'name': name,\n",
    "                'position': position\n",
    "            }\n",
    "            staffs.append(_staff)\n",
    "    except AttributeError:\n",
    "        staffs = None\n",
    "    return {\n",
    "        'casts': casts,\n",
    "        'staffs': staffs\n",
    "    }\n",
    "\n",
    "# url を引数にしてリクエストを飛ばす、sleep の後 soup を返す\n",
    "def getSoupWithDelay(target_url):\n",
    "    # アクセスディレイのため時間計測\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = requests.get(target_url)\n",
    "    soup = bs(response.content, 'html.parser')\n",
    "\n",
    "    # 処理時間含めディレイ\n",
    "    end_time = time.time()\n",
    "    sleep_time = REQUEST_DELAY - (end_time - start_time)\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 便利関数置き場\n",
    "# 日本語を英語に変える関数\n",
    "def jpn2Eng(text):\n",
    "    cate_dict = {\n",
    "        '原題': 'origin_title',\n",
    "        '製作国': 'made_in',\n",
    "        '上映時間': 'run_time',\n",
    "        'ジャンル': 'genre',\n",
    "        '原作': 'origin',\n",
    "        '脚本': 'charge_of_scenario',\n",
    "        '音楽': 'charge_of_misic',\n",
    "        '解説': 'explanation',\n",
    "        'あらすじ': 'rundown',\n",
    "        '映画レポート': 'report'\n",
    "    }\n",
    "    return cate_dict[text]\n",
    "\n",
    "\n",
    "# csv 出力するもの\n",
    "import csv\n",
    "fieldnames = [\n",
    "    'movie_id',\n",
    "    'title',\n",
    "    'production_year',\n",
    "    'published_state',\n",
    "    'product_url',\n",
    "    'run_time',\n",
    "    'made_in',\n",
    "    'genre',\n",
    "    'origin',\n",
    "    'charge_of_scenario',\n",
    "    'charge_of_misic',\n",
    "    'image_words',\n",
    "    'review_point_avg',\n",
    "    'review_point_distribution',\n",
    "    'review_point_chart',\n",
    "    'explanation',\n",
    "    'rundown',\n",
    "    'casts',\n",
    "    'staffs',\n",
    "    'scraped_at'\n",
    "]\n",
    "\n",
    "import os\n",
    "# 辞書配列からcsv出力する, 既存ファイルがあればヘッダーはつけない.\n",
    "def writeCsvWithDictList(output, filename=TARGET_FILE):\n",
    "    # 現在時刻をスクレイピングした日時として追加\n",
    "    list(map(lambda s: s.update({'scraped_at': getStrTime()}), output))\n",
    "    \n",
    "    path = './' + filename\n",
    "    with open(filename, 'a', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames, extrasaction='ignore')\n",
    "        if not os.path.exitsts(path):\n",
    "            # 初回の書き込みなのでヘッダーをつける\n",
    "            writer.writeheader()\n",
    "        writer.writerows(output)\n",
    "\n",
    "# list か dict ならJSONにして、違ったらそのまま返す関数\n",
    "import json\n",
    "def convert2Json(covar):\n",
    "    if type(covar) == dict or type(covar) == list:\n",
    "        return json.dumps(covar, ensure_ascii=False)\n",
    "    else:\n",
    "        return covar\n",
    "\n",
    "# 現在時刻取得\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "def getStrTime():\n",
    "    TIME_FORMAT = \"{0:%Y-%m-%d %H:%M:%S}\"\n",
    "    time_str = TIME_FORMAT.format(datetime.datetime.now())\n",
    "    return time_str\n",
    "\n",
    "# datetime object にする場合\n",
    "# dateutil.parser.parse(time_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reaggregateOnAirByCsv(filename):\n",
    "    url_list = getUrlListByCsv(filename, ON_AIR)\n",
    "    if len(url_list) == 0:\n",
    "        print('NotFound: OnAir movies in ', 'filename')\n",
    "        return False\n",
    "    reaggregate(url_list)\n",
    "    return True\n",
    "def reaggregateComingSoonByCsv(filename):\n",
    "    url_list = getUrlListByCsv(filename, COMINT_SOON)\n",
    "    if len(url_list) == 0:\n",
    "        print('NotFound: ComingSoon movies in ', 'filename')\n",
    "        return False\n",
    "    reaggregate(url_list)\n",
    "    return True\n",
    "def reaggregateNotProductByCsv(filename):\n",
    "    url_list = getUrlListByCsv(filename, NOT_PRODUCT)\n",
    "    if len(url_list) == 0:\n",
    "        print('NotFound: NotProduct movies in ', 'filename')\n",
    "        return False\n",
    "    reaggregate(url_list)\n",
    "    return True\n",
    "def reaggregateClosedByCsv(filename):\n",
    "    url_list = getUrlListByCsv(filename, CLOSED)\n",
    "    if len(url_list) == 0:\n",
    "        print('NotFound: Closed movies in ', 'filename')\n",
    "        return False\n",
    "    reaggregate(url_list)\n",
    "    return True\n",
    "\n",
    "def getUrlListByCsv(filename, target_state):\n",
    "    # csv から published_state が target_state の product_url をリストにする \n",
    "    df = pd.read_csv(TARGET_FILE)\n",
    "    urls = df[df.published_state == '{}'.format(target_state)]['product_url']\n",
    "    list(urls)\n",
    "    return urls\n",
    "\n",
    "def reaggregate(url_list):\n",
    "    product_list = []\n",
    "    for target_url in url_list:\n",
    "        product = parseProductWithNestByUrl(target_url)\n",
    "        product_list.append(product)\n",
    "    \n",
    "    # 一旦 CSV に出力する\n",
    "    # DB に入れるなら上書きしていく\n",
    "    writeCsvWithDictList(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全件取得\n",
    "# page 指定で途中からも可能\n",
    "def allScrapings(page=1):\n",
    "    is_next = True\n",
    "    error_count = 0\n",
    "    while is_next:\n",
    "        try:\n",
    "            products, existNextPage = scrapingTop(page, False)\n",
    "            print('page: ', page, 'isNext: ', existNextPage)\n",
    "            writeCsvWithDictList(products.values())\n",
    "\n",
    "            if existNextPage:\n",
    "                page += 1\n",
    "            else:\n",
    "                is_next = False\n",
    "        except e:\n",
    "            error_count += 1\n",
    "        \n",
    "        if error_count >= 10:\n",
    "            # Error が10回以上出たらとりあえず止める\n",
    "            print(\"ErrorCount: \", error_count)\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('FIIIIIIIINIIIIIIIIIIIISH')\n",
    "    return True\n",
    "\n",
    "def reaggregateNotColsedByCsv(filename=TARGET_FILE):\n",
    "    on_air = reaggregateOnAirByCsv(filename)\n",
    "    coming_soon = reaggregateComingSoonByCsv(filename)\n",
    "    not_product = reaggregateNotProductByCsv(filename)\n",
    "    \n",
    "    print('OnAir: ', on_air)\n",
    "    print('ComingSoon: ', coming_soon)\n",
    "    print('NotProduct: ', not_product)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 映画の重複削除\n",
    "# 後更新を優先\n",
    "def deleteDuplicatesMovie(filename=TARGET_FILE):\n",
    "    try:\n",
    "        df.sort_values('scraped_at', ascending=False).drop_duplicates(subset='movie_id')\n",
    "    except e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "147px",
    "left": "838px",
    "right": "20px",
    "top": "120px",
    "width": "393px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
